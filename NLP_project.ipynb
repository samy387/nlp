{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZK45cRqmrtN",
        "outputId": "b7af5dae-2aec-4191-86c8-93315174d944"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in c:\\users\\hp\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (4.57.1)\n",
            "Collecting datasets\n",
            "  Using cached datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in c:\\users\\hp\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\hp\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\hp\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from transformers) (2.3.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hp\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in c:\\users\\hp\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\hp\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\hp\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hp\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hp\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\hp\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from datasets) (21.0.0)\n",
            "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
            "  Using cached dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in c:\\users\\hp\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from datasets) (2.3.3)\n",
            "Collecting httpx<1.0.0 (from datasets)\n",
            "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: xxhash in c:\\users\\hp\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from datasets) (3.6.0)\n",
            "Collecting multiprocess<0.70.19 (from datasets)\n",
            "  Using cached multiprocess-0.70.18-py313-none-any.whl.metadata (7.2 kB)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
            "  Using cached aiohttp-3.13.2-cp313-cp313-win_amd64.whl.metadata (8.4 kB)\n",
            "Collecting anyio (from httpx<1.0.0->datasets)\n",
            "  Using cached anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: certifi in c:\\users\\hp\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
            "Collecting httpcore==1.* (from httpx<1.0.0->datasets)\n",
            "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: idna in c:\\users\\hp\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Collecting h11>=0.16 (from httpcore==1.*->httpx<1.0.0->datasets)\n",
            "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
            "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
            "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hp\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
            "  Using cached frozenlist-1.8.0-cp313-cp313-win_amd64.whl.metadata (21 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
            "  Using cached multidict-6.7.0-cp313-cp313-win_amd64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\hp\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
            "  Using cached yarl-1.22.0-cp313-cp313-win_amd64.whl.metadata (77 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hp\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\hp\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\users\\hp\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Using cached datasets-4.4.1-py3-none-any.whl (511 kB)\n",
            "Using cached dill-0.4.0-py3-none-any.whl (119 kB)\n",
            "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
            "Using cached multiprocess-0.70.18-py313-none-any.whl (151 kB)\n",
            "Using cached aiohttp-3.13.2-cp313-cp313-win_amd64.whl (452 kB)\n",
            "Using cached multidict-6.7.0-cp313-cp313-win_amd64.whl (45 kB)\n",
            "Using cached yarl-1.22.0-cp313-cp313-win_amd64.whl (86 kB)\n",
            "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
            "Using cached frozenlist-1.8.0-cp313-cp313-win_amd64.whl (43 kB)\n",
            "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
            "Using cached anyio-4.11.0-py3-none-any.whl (109 kB)\n",
            "Installing collected packages: multidict, h11, frozenlist, dill, anyio, aiohappyeyeballs, yarl, multiprocess, httpcore, aiosignal, httpx, aiohttp, datasets\n",
            "\n",
            "   --- ------------------------------------  1/13 [h11]\n",
            "   --- ------------------------------------  1/13 [h11]\n",
            "   --- ------------------------------------  1/13 [h11]\n",
            "   --------- ------------------------------  3/13 [dill]\n",
            "   --------- ------------------------------  3/13 [dill]\n",
            "   --------- ------------------------------  3/13 [dill]\n",
            "   --------- ------------------------------  3/13 [dill]\n",
            "   --------- ------------------------------  3/13 [dill]\n",
            "   --------- ------------------------------  3/13 [dill]\n",
            "   --------- ------------------------------  3/13 [dill]\n",
            "   --------- ------------------------------  3/13 [dill]\n",
            "   --------- ------------------------------  3/13 [dill]\n",
            "   --------- ------------------------------  3/13 [dill]\n",
            "   ------------ ---------------------------  4/13 [anyio]\n",
            "   ------------ ---------------------------  4/13 [anyio]\n",
            "   ------------ ---------------------------  4/13 [anyio]\n",
            "   ------------ ---------------------------  4/13 [anyio]\n",
            "   ------------ ---------------------------  4/13 [anyio]\n",
            "   ------------ ---------------------------  4/13 [anyio]\n",
            "   ------------ ---------------------------  4/13 [anyio]\n",
            "   ------------------ ---------------------  6/13 [yarl]\n",
            "   ------------------ ---------------------  6/13 [yarl]\n",
            "   --------------------- ------------------  7/13 [multiprocess]\n",
            "   --------------------- ------------------  7/13 [multiprocess]\n",
            "   --------------------- ------------------  7/13 [multiprocess]\n",
            "   --------------------- ------------------  7/13 [multiprocess]\n",
            "   --------------------- ------------------  7/13 [multiprocess]\n",
            "   --------------------- ------------------  7/13 [multiprocess]\n",
            "   --------------------- ------------------  7/13 [multiprocess]\n",
            "   --------------------- ------------------  7/13 [multiprocess]\n",
            "   --------------------- ------------------  7/13 [multiprocess]\n",
            "   ------------------------ ---------------  8/13 [httpcore]\n",
            "   ------------------------ ---------------  8/13 [httpcore]\n",
            "   ------------------------ ---------------  8/13 [httpcore]\n",
            "   ------------------------ ---------------  8/13 [httpcore]\n",
            "   ------------------------ ---------------  8/13 [httpcore]\n",
            "   --------------------------- ------------  9/13 [aiosignal]\n",
            "   ------------------------------ --------- 10/13 [httpx]\n",
            "   ------------------------------ --------- 10/13 [httpx]\n",
            "   ------------------------------ --------- 10/13 [httpx]\n",
            "   ------------------------------ --------- 10/13 [httpx]\n",
            "   ------------------------------ --------- 10/13 [httpx]\n",
            "   ------------------------------ --------- 10/13 [httpx]\n",
            "   --------------------------------- ------ 11/13 [aiohttp]\n",
            "   --------------------------------- ------ 11/13 [aiohttp]\n",
            "   --------------------------------- ------ 11/13 [aiohttp]\n",
            "   --------------------------------- ------ 11/13 [aiohttp]\n",
            "   --------------------------------- ------ 11/13 [aiohttp]\n",
            "   --------------------------------- ------ 11/13 [aiohttp]\n",
            "   --------------------------------- ------ 11/13 [aiohttp]\n",
            "   --------------------------------- ------ 11/13 [aiohttp]\n",
            "   --------------------------------- ------ 11/13 [aiohttp]\n",
            "   --------------------------------- ------ 11/13 [aiohttp]\n",
            "   --------------------------------- ------ 11/13 [aiohttp]\n",
            "   --------------------------------- ------ 11/13 [aiohttp]\n",
            "   ------------------------------------ --- 12/13 [datasets]\n",
            "   ------------------------------------ --- 12/13 [datasets]\n",
            "   ------------------------------------ --- 12/13 [datasets]\n",
            "   ------------------------------------ --- 12/13 [datasets]\n",
            "   ------------------------------------ --- 12/13 [datasets]\n",
            "   ------------------------------------ --- 12/13 [datasets]\n",
            "   ------------------------------------ --- 12/13 [datasets]\n",
            "   ------------------------------------ --- 12/13 [datasets]\n",
            "   ------------------------------------ --- 12/13 [datasets]\n",
            "   ------------------------------------ --- 12/13 [datasets]\n",
            "   ------------------------------------ --- 12/13 [datasets]\n",
            "   ------------------------------------ --- 12/13 [datasets]\n",
            "   ------------------------------------ --- 12/13 [datasets]\n",
            "   ------------------------------------ --- 12/13 [datasets]\n",
            "   ------------------------------------ --- 12/13 [datasets]\n",
            "   ------------------------------------ --- 12/13 [datasets]\n",
            "   ------------------------------------ --- 12/13 [datasets]\n",
            "   ------------------------------------ --- 12/13 [datasets]\n",
            "   ------------------------------------ --- 12/13 [datasets]\n",
            "   ------------------------------------ --- 12/13 [datasets]\n",
            "   ------------------------------------ --- 12/13 [datasets]\n",
            "   ------------------------------------ --- 12/13 [datasets]\n",
            "   ------------------------------------ --- 12/13 [datasets]\n",
            "   ---------------------------------------- 13/13 [datasets]\n",
            "\n",
            "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 anyio-4.11.0 datasets-4.4.1 dill-0.4.0 frozenlist-1.8.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 multidict-6.7.0 multiprocess-0.70.18 yarl-1.22.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qB6QpxXaTIP",
        "outputId": "ec287944-b5fd-4373-d610-17b59ab21b23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                Text  Amount Category\n",
            "0               I bought lunch and paid $111 at work     111     Food\n",
            "1  I grabbed a quick bite at sandwich shop and pa...      37     Food\n",
            "2  I purchased ingredients and spent $138 for coo...     138     Food\n",
            "3      I had takeout and paid $115 at the McDonald's     115     Food\n",
            "4                 I spent $128 on food for the party     128     Food\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"expense_dataset_10000.csv\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zpjkMdXNbDkl"
      },
      "outputs": [],
      "source": [
        "df['Text'] = df['Text'].str.lower()\n",
        "df['Text'] = df['Text'].str.replace(r'[^a-zA-Z0-9\\s$\\'\\-]', '', regex=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJxf44U1bHDj",
        "outputId": "9c7e7a36-3c78-40f1-9c1a-dc6b9080d1f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Clothing' 'Entertainment' 'Food' 'Health' 'Transport']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "df['Category_encoded'] = le.fit_transform(df['Category'])\n",
        "print(le.classes_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mhsSkMCrbOny"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df['Text'], df['Category_encoded'], test_size=0.2, random_state=42, stratify=df['Category_encoded']\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Dn3QhYWQKNmC"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hp\\OneDrive\\Desktop\\nlp\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import DistilBertTokenizerFast\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6qhaDQ9nKb3r"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\hp\\OneDrive\\Desktop\\nlp\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hp\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        }
      ],
      "source": [
        "#Loading a pretrained tokenizer (distilbert)\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "EK3y3l8nKjnu"
      },
      "outputs": [],
      "source": [
        "#This converts text and labels into tensors that the model can train on\n",
        "\n",
        "class ExpenseDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=64):\n",
        "        self.texts = texts.tolist()\n",
        "        self.labels = labels.tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Remove the extra batch dimension\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
        "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-2JrIMn_Kqwp"
      },
      "outputs": [],
      "source": [
        "#train/test datasets\n",
        "train_dataset = ExpenseDataset(train_texts, train_labels, tokenizer)\n",
        "test_dataset = ExpenseDataset(test_texts, test_labels, tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdshIT4mKvKi",
        "outputId": "a8344721-9989-4f33-b34c-aa6ef4fe9812"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "#loading distil bert pretrained model for classification\n",
        "from transformers import DistilBertForSequenceClassification\n",
        "\n",
        "num_labels = len(le.classes_)\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\n",
        "    'distilbert-base-uncased',\n",
        "    num_labels=num_labels\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "bAA7yqKmSttJ"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "iuCNzGdVNWmJ"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m training_args = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./results\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# where the model checkpoints will be saved\u001b[39;49;00m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# evaluate after each epoch\u001b[39;49;00m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# save after each epoch\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# recommended for fine-tuning\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# batch size for training\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# batch size for evaluation\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;66;43;03m# number of training epochs\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# regularization\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<string>:135\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, parallelism_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, project, trackio_space_id, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, hub_revision, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, liger_kernel_config, eval_use_gather_object, average_tokens_across_devices)\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Desktop\\nlp\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1811\u001b[39m, in \u001b[36mTrainingArguments.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1809\u001b[39m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[32m   1810\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[32m-> \u001b[39m\u001b[32m1811\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m   1813\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.torchdynamo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1814\u001b[39m     warnings.warn(\n\u001b[32m   1815\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`torchdynamo` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1816\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m `torch_compile_backend` instead\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1817\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m   1818\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Desktop\\nlp\\.venv\\Lib\\site-packages\\transformers\\training_args.py:2355\u001b[39m, in \u001b[36mTrainingArguments.device\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2351\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2352\u001b[39m \u001b[33;03mThe device used by this process.\u001b[39;00m\n\u001b[32m   2353\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2354\u001b[39m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m2355\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_devices\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\functools.py:1039\u001b[39m, in \u001b[36mcached_property.__get__\u001b[39m\u001b[34m(self, instance, owner)\u001b[39m\n\u001b[32m   1037\u001b[39m val = cache.get(\u001b[38;5;28mself\u001b[39m.attrname, _NOT_FOUND)\n\u001b[32m   1038\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[32m-> \u001b[39m\u001b[32m1039\u001b[39m     val = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1040\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1041\u001b[39m         cache[\u001b[38;5;28mself\u001b[39m.attrname] = val\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\OneDrive\\Desktop\\nlp\\.venv\\Lib\\site-packages\\transformers\\training_args.py:2225\u001b[39m, in \u001b[36mTrainingArguments._setup_devices\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2223\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[32m   2224\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[32m-> \u001b[39m\u001b[32m2225\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m   2226\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2227\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[33m'\u001b[39m\u001b[33maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2228\u001b[39m         )\n\u001b[32m   2229\u001b[39m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[32m   2230\u001b[39m accelerator_state_kwargs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = {\u001b[33m\"\u001b[39m\u001b[33menabled\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33muse_configured_state\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
            "\u001b[31mImportError\u001b[39m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",           # where the model checkpoints will be saved\n",
        "    eval_strategy=\"epoch\",      # evaluate after each epoch\n",
        "    save_strategy=\"epoch\",            # save after each epoch\n",
        "    learning_rate=2e-5,               # recommended for fine-tuning\n",
        "    per_device_train_batch_size=16,   # batch size for training\n",
        "    per_device_eval_batch_size=16,    # batch size for evaluation\n",
        "    num_train_epochs=5,               # number of training epochs\n",
        "    weight_decay=0.01,                # regularization\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APh8i7jTNokQ",
        "outputId": "d0546a4f-c5d6-4c3e-f514-867cb9dba6fb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1848144899.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "O97aJ3zaNp-Z",
        "outputId": "1c6d0e9d-49d3-4796-c6c7-18015b290810"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2500/2500 04:50, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.136600</td>\n",
              "      <td>0.001741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.001300</td>\n",
              "      <td>0.000526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>0.000289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>0.000202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>0.000179</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2500, training_loss=0.027926406210660934, metrics={'train_runtime': 290.3005, 'train_samples_per_second': 137.788, 'train_steps_per_second': 8.612, 'total_flos': 662372428800000.0, 'train_loss': 0.027926406210660934, 'epoch': 5.0})"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "m-UA6Zs_RCJ9",
        "outputId": "21342ccb-d19a-4ab8-e504-4d55e66b8296"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.00017930757894646376,\n",
              " 'eval_runtime': 3.8025,\n",
              " 'eval_samples_per_second': 525.968,\n",
              " 'eval_steps_per_second': 32.873,\n",
              " 'epoch': 5.0}"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "fEr7aBpgwJzO",
        "outputId": "bae46e6f-dc21-40ce-fb94-669a1be7e95f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:12]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.00017930757894646376, 'eval_runtime': 3.7499, 'eval_samples_per_second': 533.349, 'eval_steps_per_second': 33.334, 'epoch': 5.0}\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "metrics = trainer.evaluate(test_dataset)\n",
        "print(metrics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1ZGV6oEwM6T"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def predict_expense_with_amount(text):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "\n",
        "    # tokenize input\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # predict category\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    pred_class = torch.argmax(outputs.logits, dim=1).item()\n",
        "    category = le.inverse_transform([pred_class])[0]\n",
        "\n",
        "    # extract amount using regex\n",
        "    match = re.search(r'\\$?(\\d+(\\.\\d+)?)\\$?', text)  # matches 10, 10.5, $10, 10$\n",
        "    amount = float(match.group(1)) if match else None\n",
        "\n",
        "    return category, amount\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7smAIz90hFt",
        "outputId": "2f4afd7c-0bc5-4486-9788-0edb0e86b335"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence: 'i bought a coffee for 10$' -> Category: Food, Amount: $10.0\n",
            "Sentence: 'Grabbed lunch at McDonald's for $12' -> Category: Food, Amount: $12.0\n",
            "Sentence: 'Took an Uber to the airport and paid $25' -> Category: Transport, Amount: $25.0\n",
            "Sentence: 'Went to the cinema and paid $15 for tickets' -> Category: Entertainment, Amount: $15.0\n",
            "Sentence: 'Bought vitamins for $20' -> Category: Health, Amount: $20.0\n",
            "Sentence: 'Bought a birthday gift for $40' -> Category: Entertainment, Amount: $40.0\n"
          ]
        }
      ],
      "source": [
        "test_sentences = [\n",
        "    \"i bought a coffee for 10$\",\n",
        "    \"Grabbed lunch at McDonald's for $12\",\n",
        "    \"Took an Uber to the airport and paid $25\",\n",
        "    \"Went to the cinema and paid $15 for tickets\",\n",
        "    \"Bought vitamins for $20\",\n",
        "    \"Bought a birthday gift for $40\"\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    category, amount = predict_expense_with_amount(sentence)\n",
        "    print(f\"Sentence: '{sentence}' -> Category: {category}, Amount: ${amount}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sa3pXskN1LIB",
        "outputId": "3e1b543e-746a-4e79-8f0e-229c649559fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overall Accuracy: 1.0\n",
            "\n",
            "Classification Report:\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "     Clothing       1.00      1.00      1.00       400\n",
            "Entertainment       1.00      1.00      1.00       400\n",
            "         Food       1.00      1.00      1.00       400\n",
            "       Health       1.00      1.00      1.00       400\n",
            "    Transport       1.00      1.00      1.00       400\n",
            "\n",
            "     accuracy                           1.00      2000\n",
            "    macro avg       1.00      1.00      1.00      2000\n",
            " weighted avg       1.00      1.00      1.00      2000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import torch\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "# Make sure the model is on the correct device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "for batch in test_dataset:\n",
        "    # Move batch tensors to device\n",
        "    input_ids = batch[\"input_ids\"].unsqueeze(0).to(device)\n",
        "    attention_mask = batch[\"attention_mask\"].unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pred = torch.argmax(outputs.logits, dim=1).item()\n",
        "\n",
        "    all_preds.append(pred)\n",
        "    all_labels.append(batch[\"labels\"].item())\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(\"Overall Accuracy:\", accuracy)\n",
        "\n",
        "# Precision, Recall, F1-score per class\n",
        "report = classification_report(all_labels, all_preds, target_names=le.classes_)\n",
        "print(\"\\nClassification Report:\\n\", report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLu13ZGE943q",
        "outputId": "c3121467-efcb-4686-90eb-cfff88e65934"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('expense_classifier_tokenizer/tokenizer_config.json',\n",
              " 'expense_classifier_tokenizer/special_tokens_map.json',\n",
              " 'expense_classifier_tokenizer/vocab.txt',\n",
              " 'expense_classifier_tokenizer/added_tokens.json',\n",
              " 'expense_classifier_tokenizer/tokenizer.json')"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save the model\n",
        "model.save_pretrained(\"expense_classifier_model\")\n",
        "\n",
        "# Save the tokenizer\n",
        "tokenizer.save_pretrained(\"expense_classifier_tokenizer\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
